{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce7384e-256e-49e3-ac32-0941bf05598a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 14:47:35,529 - INFO - ============================================================\n",
      "2025-07-06 14:47:35,530 - INFO - STARTING OPTIMIZED BASKETBALL ANALYTICS PROCESSING\n",
      "2025-07-06 14:47:35,531 - INFO - ============================================================\n",
      "2025-07-06 14:47:35,531 - INFO - Phase 1: Generating regular combination statistics...\n",
      "2025-07-06 14:47:35,532 - INFO - Starting parallel combination generation for year 2025\n",
      "2025-07-06 14:47:35,533 - INFO - Parameters: ps=False, vs=False, sizes=[2, 3, 4]\n",
      "2025-07-06 14:47:35,565 - INFO - Found 30 unique teams for year 2025\n",
      "2025-07-06 14:47:35,566 - INFO - Collecting all possible combinations...\n",
      "Collecting combinations: 100%|█████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 525.35it/s]\n",
      "2025-07-06 14:47:35,627 - INFO - Found 5239 combinations of size 2\n",
      "2025-07-06 14:47:35,628 - INFO - Found 19050 combinations of size 3\n",
      "2025-07-06 14:47:35,629 - INFO - Found 29304 combinations of size 4\n",
      "2025-07-06 14:47:35,630 - INFO - Processing size 2 combinations...\n",
      "2025-07-06 14:47:35,639 - INFO - Starting parallel processing for size 2 with 21 workers\n",
      "2025-07-06 14:47:35,640 - INFO - Processing 630 work units\n",
      "Processing size 2:   3%|██                                                             | 21/630 [00:00<00:09, 66.91it/s]2025-07-06 14:47:36,150 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:47:36,173 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:47:36,174 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "Processing size 2:   7%|████▏                                                         | 42/630 [00:00<00:04, 118.54it/s]2025-07-06 14:47:36,179 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:47:36,180 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:47:36,190 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:47:36,192 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:47:36,196 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:47:36,197 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:47:36,198 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:47:36,201 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:47:36,217 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:47:36,226 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:47:36,234 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:47:36,240 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:47:36,270 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:47:36,287 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "Processing size 2:  10%|██████▍                                                       | 65/630 [00:00<00:03, 151.12it/s]2025-07-06 14:47:36,318 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:47:36,366 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:47:36,379 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "Processing size 2:  15%|█████████▍                                                     | 94/630 [00:00<00:06, 87.54it/s]2025-07-06 14:47:36,834 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "Processing size 2:  58%|████████████████████████████████████▏                         | 368/630 [00:05<00:07, 35.93it/s]2025-07-06 14:47:41,297 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:47:41,364 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:47:41,380 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "Processing size 2:  60%|█████████████████████████████████████▏                        | 378/630 [00:05<00:05, 48.26it/s]2025-07-06 14:47:41,410 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:47:41,463 - ERROR - Error in parallel processing for size 2: unsupported operand type(s) for +: 'int' and 'str'\n",
      "Processing size 2:  61%|█████████████████████████████████████▊                        | 384/630 [00:05<00:04, 50.57it/s]2025-07-06 14:47:41,530 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "Processing size 2:  62%|██████████████████████████████████████▍                       | 390/630 [00:05<00:05, 46.17it/s]2025-07-06 14:47:41,660 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:47:41,686 - ERROR - Error in parallel processing for size 2: can only concatenate str (not \"int\") to str\n",
      "Processing size 2: 100%|██████████████████████████████████████████████████████████████| 630/630 [00:14<00:00, 43.82it/s]\n",
      "2025-07-06 14:47:50,197 - INFO - Size 2 processing complete. Generated 5007 results\n",
      "2025-07-06 14:48:01,936 - INFO - Size 2 dataset shape: (5007, 215)\n",
      "2025-07-06 14:48:01,938 - INFO - Processing size 3 combinations...\n",
      "2025-07-06 14:48:01,967 - INFO - Starting parallel processing for size 3 with 21 workers\n",
      "2025-07-06 14:48:01,968 - INFO - Processing 630 work units\n",
      "Processing size 3:   4%|██▌                                                            | 26/630 [00:02<00:38, 15.52it/s]2025-07-06 14:48:05,674 - ERROR - Error in parallel processing for size 3: can only concatenate str (not \"int\") to str\n",
      "Processing size 3:   5%|███                                                            | 31/630 [00:02<00:38, 15.57it/s]2025-07-06 14:48:06,010 - ERROR - Error in parallel processing for size 3: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:48:06,047 - ERROR - Error in parallel processing for size 3: can only concatenate str (not \"int\") to str\n",
      "Processing size 3:   6%|███▋                                                           | 37/630 [00:02<00:25, 23.59it/s]2025-07-06 14:48:06,208 - ERROR - Error in parallel processing for size 3: can only concatenate str (not \"int\") to str\n",
      "Processing size 3:   6%|████                                                           | 40/630 [00:02<00:23, 24.90it/s]2025-07-06 14:48:06,469 - ERROR - Error in parallel processing for size 3: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:48:06,542 - ERROR - Error in parallel processing for size 3: can only concatenate str (not \"int\") to str\n",
      "Processing size 3:   7%|████▌                                                          | 46/630 [00:03<00:35, 16.54it/s]2025-07-06 14:48:06,884 - ERROR - Error in parallel processing for size 3: can only concatenate str (not \"int\") to str\n",
      "Processing size 3:   8%|█████                                                          | 50/630 [00:03<00:37, 15.49it/s]2025-07-06 14:48:07,019 - ERROR - Error in parallel processing for size 3: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:48:07,227 - ERROR - Error in parallel processing for size 3: can only concatenate str (not \"int\") to str\n",
      "Processing size 3:   8%|█████▎                                                         | 53/630 [00:03<00:38, 15.06it/s]2025-07-06 14:48:07,327 - ERROR - Error in parallel processing for size 3: can only concatenate str (not \"int\") to str\n",
      "Processing size 3:   9%|█████▌                                                         | 56/630 [00:04<00:32, 17.78it/s]2025-07-06 14:48:07,333 - ERROR - Error in parallel processing for size 3: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:48:07,401 - ERROR - Error in parallel processing for size 3: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:48:07,541 - ERROR - Error in parallel processing for size 3: can only concatenate str (not \"int\") to str\n",
      "Processing size 3:   9%|█████▉                                                         | 59/630 [00:04<00:34, 16.50it/s]2025-07-06 14:48:07,586 - ERROR - Error in parallel processing for size 3: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:48:07,994 - ERROR - Error in parallel processing for size 3: can only concatenate str (not \"int\") to str\n",
      "Processing size 3:  10%|██████▏                                                        | 62/630 [00:04<00:50, 11.29it/s]2025-07-06 14:48:08,102 - ERROR - Error in parallel processing for size 3: can only concatenate str (not \"int\") to str\n",
      "Processing size 3:  10%|██████▍                                                        | 64/630 [00:04<00:48, 11.77it/s]2025-07-06 14:48:08,146 - ERROR - Error in parallel processing for size 3: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:48:08,218 - ERROR - Error in parallel processing for size 3: can only concatenate str (not \"int\") to str\n",
      "Processing size 3:  11%|██████▋                                                        | 67/630 [00:04<00:39, 14.40it/s]2025-07-06 14:48:08,287 - ERROR - Error in parallel processing for size 3: can only concatenate str (not \"int\") to str\n",
      "Processing size 3:  11%|███████                                                        | 70/630 [00:05<00:33, 16.70it/s]2025-07-06 14:48:08,459 - ERROR - Error in parallel processing for size 3: can only concatenate str (not \"int\") to str\n",
      "Processing size 3:  14%|████████▊                                                      | 88/630 [00:06<00:34, 15.81it/s]2025-07-06 14:48:09,830 - ERROR - Error in parallel processing for size 3: can only concatenate str (not \"int\") to str\n",
      "Processing size 3:  57%|███████████████████████████████████▌                          | 361/630 [00:35<01:09,  3.85it/s]2025-07-06 14:48:38,797 - ERROR - Error in parallel processing for size 3: can only concatenate str (not \"int\") to str\n",
      "Processing size 3:  60%|█████████████████████████████████████▍                        | 381/630 [00:36<00:15, 16.55it/s]2025-07-06 14:48:39,390 - ERROR - Error in parallel processing for size 3: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:48:39,392 - ERROR - Error in parallel processing for size 3: unsupported operand type(s) for +: 'int' and 'str'\n",
      "Processing size 3:  61%|█████████████████████████████████████▊                        | 384/630 [00:36<00:13, 18.50it/s]2025-07-06 14:48:39,505 - ERROR - Error in parallel processing for size 3: unsupported operand type(s) for +: 'int' and 'str'\n",
      "2025-07-06 14:48:39,513 - ERROR - Error in parallel processing for size 3: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:48:39,514 - ERROR - Error in parallel processing for size 3: can only concatenate str (not \"int\") to str\n",
      "2025-07-06 14:48:39,577 - ERROR - Error in parallel processing for size 3: unsupported operand type(s) for +: 'int' and 'str'\n",
      "Processing size 3:  62%|██████████████████████████████████████▎                       | 389/630 [00:36<00:13, 18.08it/s]2025-07-06 14:48:39,969 - ERROR - Error in parallel processing for size 3: can only concatenate str (not \"int\") to str\n",
      "Processing size 3: 100%|██████████████████████████████████████████████████████████████| 630/630 [01:08<00:00,  9.15it/s]\n",
      "2025-07-06 14:49:12,458 - INFO - Size 3 processing complete. Generated 18305 results\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Set up logging for detailed progress tracking\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('basketball_analytics.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def extract_player_ids(entity_id):\n",
    "    \"\"\"Extract player IDs from EntityId string\"\"\"\n",
    "    # Ensure entity_id is a string\n",
    "    entity_id_str = str(entity_id)\n",
    "    return entity_id_str.split('-')\n",
    "\n",
    "def generate_combinations_vectorized(entity_ids, sizes=[2, 3, 4]):\n",
    "    \"\"\"Generate all combinations using vectorized operations where possible\"\"\"\n",
    "    combo_to_rows = defaultdict(list)\n",
    "    \n",
    "    for idx, entity_id in enumerate(entity_ids):\n",
    "        # Ensure entity_id is a string\n",
    "        entity_id_str = str(entity_id)\n",
    "        player_ids = entity_id_str.split('-')\n",
    "        \n",
    "        # Generate combinations for all sizes at once\n",
    "        for size in sizes:\n",
    "            if len(player_ids) >= size:\n",
    "                for combo in combinations(player_ids, size):\n",
    "                    # Ensure all player IDs are strings before sorting and joining\n",
    "                    combo_strs = [str(pid) for pid in combo]\n",
    "                    key = '-'.join(sorted(combo_strs))\n",
    "                    combo_to_rows[key].append(idx)\n",
    "    \n",
    "    return combo_to_rows\n",
    "\n",
    "def precompute_basketball_calculations(df):\n",
    "    \"\"\"Precompute all basketball-related calculations once\"\"\"\n",
    "    # Calculate all derived fields in one pass\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Basic calculations\n",
    "    df['FGA'] = df['FG2A'] + df['FG3A']\n",
    "    df['FGM'] = df['FG2M'] + df['FG3M']\n",
    "    df['opp_FGA'] = df['opp_FG2A'] + df['opp_FG3A']\n",
    "    df['opp_FGM'] = df['opp_FG2M'] + df['opp_FG3M']\n",
    "    \n",
    "    # Calculate all miss columns at once\n",
    "    miss_calculations = {\n",
    "        'two_point_misses': ('FG2A', 'FG2M'),\n",
    "        'opp_two_point_misses': ('opp_FG2A', 'opp_FG2M'),\n",
    "        'at_rim_misses': ('AtRimFGA', 'AtRimFGM'),\n",
    "        'opp_at_rim_misses': ('opp_AtRimFGA', 'opp_AtRimFGM'),\n",
    "        'short_midrange_misses': ('ShortMidRangeFGA', 'ShortMidRangeFGM'),\n",
    "        'opp_short_midrange_misses': ('opp_ShortMidRangeFGA', 'opp_ShortMidRangeFGM'),\n",
    "        'long_midrange_misses': ('LongMidRangeFGA', 'LongMidRangeFGM'),\n",
    "        'opp_long_midrange_misses': ('opp_LongMidRangeFGA', 'opp_LongMidRangeFGM'),\n",
    "        'corner3_misses': ('Corner3FGA', 'Corner3FGM'),\n",
    "        'opp_corner3_misses': ('opp_Corner3FGA', 'opp_Corner3FGM'),\n",
    "        'arc3_misses': ('Arc3FGA', 'Arc3FGM'),\n",
    "        'opp_arc3_misses': ('opp_Arc3FGA', 'opp_Arc3FGM'),\n",
    "        'ft_misses': ('FTA', 'FtPoints'),\n",
    "        'opp_ft_misses': ('opp_FTA', 'opp_FtPoints'),\n",
    "        'fg_misses': ('FGA', 'FGM'),\n",
    "        'opp_fg_misses': ('opp_FGA', 'opp_FGM')\n",
    "    }\n",
    "    \n",
    "    for miss_col, (attempts, makes) in miss_calculations.items():\n",
    "        if attempts in df.columns and makes in df.columns:\n",
    "            df[miss_col] = df[attempts] - df[makes]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_team_data_optimized(team_id, year, ps=False, vs=False):\n",
    "    \"\"\"Optimized team data processing with caching\"\"\"\n",
    "    # Ensure team_id is a string for consistent cache naming\n",
    "    team_id_str = str(team_id)\n",
    "    cache_key = f\"{team_id_str}_{year}_{ps}_{vs}\"\n",
    "    cache_file = f\"cache/{cache_key}.pkl\"\n",
    "    \n",
    "    # Check cache first\n",
    "    if os.path.exists(cache_file):\n",
    "        try:\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        except:\n",
    "            pass  # Cache corrupted, rebuild\n",
    "    \n",
    "    try:\n",
    "        pstring = \"_ps\" if ps else \"\"\n",
    "        \n",
    "        # Load data - use team_id_str for file paths\n",
    "        if vs == False:\n",
    "            df1 = pd.read_csv(f\"data/{year}/{team_id_str}{pstring}.csv\")\n",
    "            df2 = pd.read_csv(f\"data/{year}/{team_id_str}_vs{pstring}.csv\")\n",
    "        else:\n",
    "            df2 = pd.read_csv(f\"data/{year}/{team_id_str}{pstring}.csv\")\n",
    "            df1 = pd.read_csv(f\"data/{year}/{team_id_str}_vs{pstring}.csv\")\n",
    "        \n",
    "        # Merge data efficiently\n",
    "        df2 = df2.drop(columns=['team_vs'], errors='ignore')\n",
    "        \n",
    "        # Rename opponent columns in one operation\n",
    "        opp_rename = {col: f'opp_{col}' for col in df2.columns if col != 'EntityId'}\n",
    "        df2 = df2.rename(columns=opp_rename)\n",
    "        \n",
    "        # Merge\n",
    "        df = pd.merge(df1, df2, on='EntityId', how='left')\n",
    "        df = df.fillna(0)\n",
    "        \n",
    "        # Precompute all basketball calculations\n",
    "        df = precompute_basketball_calculations(df)\n",
    "        \n",
    "        # Generate combinations\n",
    "        combo_to_rows = generate_combinations_vectorized(df['EntityId'].values)\n",
    "        \n",
    "        # Cache the result\n",
    "        os.makedirs('cache', exist_ok=True)\n",
    "        with open(cache_file, 'wb') as f:\n",
    "            pickle.dump((df, combo_to_rows), f)\n",
    "        \n",
    "        return df, combo_to_rows\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing team {team_id}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def compute_combo_stats_vectorized(df_subset, combo_key):\n",
    "    \"\"\"Vectorized computation of combination statistics\"\"\"\n",
    "    if len(df_subset) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Define column groups\n",
    "    id_cols = ['EntityId', 'TeamId', 'Name', 'ShortName', 'RowId', 'TeamAbbreviation', \n",
    "               'team_id', 'year', 'season', 'team_vs']\n",
    "    pct_cols = [col for col in df_subset.columns if 'pct' in col.lower()]\n",
    "    sum_cols = [col for col in df_subset.columns if col not in id_cols and col not in pct_cols]\n",
    "    \n",
    "    # Aggregate by summing (vectorized)\n",
    "    sums = df_subset[sum_cols].sum().to_frame().T\n",
    "    \n",
    "    # Calculate percentages efficiently\n",
    "    newframe = calculate_basketball_percentages_vectorized(sums)\n",
    "    \n",
    "    # Calculate weighted averages in batch\n",
    "    weight_mapping = {\n",
    "        'DefTwoPtReboundPct': 'opp_two_point_misses',\n",
    "        'OffTwoPtReboundPct': 'two_point_misses',\n",
    "        'DefThreePtReboundPct': 'opp_FG3A',\n",
    "        'DefFGReboundPct': 'opp_fg_misses',\n",
    "        'OffFGReboundPct': 'fg_misses',\n",
    "        'OffLongMidRangeReboundPct': 'long_midrange_misses',\n",
    "        'DefLongMidRangeReboundPct': 'opp_long_midrange_misses',\n",
    "        'OffThreePtReboundPct': 'opp_FG3A',\n",
    "        'OffArc3ReboundPct': 'arc3_misses',\n",
    "        'DefArc3ReboundPct': 'opp_arc3_misses',\n",
    "        'DefAtRimReboundPct': 'opp_at_rim_misses',\n",
    "        'DefShortMidRangeReboundPct': 'opp_short_midrange_misses',\n",
    "        'DefCorner3ReboundPct': 'opp_corner3_misses',\n",
    "        'OffAtRimReboundPct': 'at_rim_misses',\n",
    "        'SelfORebPct': 'fg_misses',\n",
    "        'OffShortMidRangeReboundPct': 'short_midrange_misses',\n",
    "        'OffCorner3ReboundPct': 'corner3_misses',\n",
    "        'SecondChanceTsPct': 'SecondChanceOffPoss',\n",
    "        'SecondChanceCorner3PctAssisted': 'SecondChanceCorner3FGM',\n",
    "        'SecondChanceArc3PctAssisted': 'SecondChanceArc3FGM',\n",
    "        'SecondChanceAtRimPctAssisted': 'SecondChanceAtRimFGM'\n",
    "    }\n",
    "    \n",
    "    # Calculate weighted averages efficiently\n",
    "    for metric, weight_col in weight_mapping.items():\n",
    "        if metric in df_subset.columns and weight_col in df_subset.columns:\n",
    "            total_weight = df_subset[weight_col].sum()\n",
    "            if total_weight > 0:\n",
    "                weighted_sum = (df_subset[metric] * df_subset[weight_col]).sum()\n",
    "                newframe[metric] = weighted_sum / total_weight\n",
    "            else:\n",
    "                newframe[metric] = 0\n",
    "        else:\n",
    "            newframe[metric] = 0\n",
    "    \n",
    "    # Clean up opponent columns\n",
    "    opp_cols = [col for col in newframe.columns if col.startswith('opp_')]\n",
    "    newframe = newframe.drop(columns=opp_cols)\n",
    "    \n",
    "    # Add metadata\n",
    "    newframe['combo_key'] = combo_key\n",
    "    newframe['combo_size'] = len(str(combo_key).split('-'))\n",
    "    \n",
    "    return newframe\n",
    "\n",
    "def calculate_basketball_percentages_vectorized(df):\n",
    "    \"\"\"Vectorized calculation of basketball percentages\"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    # Use numpy operations for speed\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        result['Fg3Pct'] = np.where(result['FG3A'] > 0, result['FG3M'] / result['FG3A'], 0)\n",
    "        result['Fg2Pct'] = np.where(result['FG2A'] > 0, result['FG2M'] / result['FG2A'], 0)\n",
    "        result['FGA'] = result['FG2A'] + result['FG3A']\n",
    "        result['FGM'] = result['FG2M'] + result['FG3M']\n",
    "        result['PenaltyFGA'] = result['PenaltyFG2A'] + result['PenaltyFG3A']\n",
    "        result['SecondChanceFGA'] = result['SecondChanceFG2A'] + result['SecondChanceFG3A']\n",
    "        \n",
    "        # NonHeaveFg3Pct calculation\n",
    "        heave_adjusted_3pa = result['FG3A'] - result['HeaveAttempts']\n",
    "        result['NonHeaveFg3Pct'] = np.where(heave_adjusted_3pa > 0, \n",
    "                                          result['FG3M'] / heave_adjusted_3pa, 0)\n",
    "        \n",
    "        # EfgPct calculation\n",
    "        result['EfgPct'] = np.where(result['FGA'] > 0,\n",
    "                                  (result['FG2M'] + 1.5 * result['FG3M']) / result['FGA'], 0)\n",
    "        \n",
    "        # True Shooting Percentage\n",
    "        and1_2pt = result['2pt And 1 Free Throw Trips']\n",
    "        and1_3pt = result['3pt And 1 Free Throw Trips']\n",
    "        \n",
    "        w = np.where(result['FTA'] > 0,\n",
    "                    (and1_2pt + 1.5 * and1_3pt + 0.44 * (result['FTA'] - and1_2pt - and1_3pt)) / result['FTA'],\n",
    "                    0.44)\n",
    "        \n",
    "        tss_denominator = 2 * (result['FGA'] + w * result['FTA'])\n",
    "        result['TsPct'] = np.where(tss_denominator > 0,\n",
    "                                 result['Points'] / tss_denominator, 0)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def process_combinations_parallel(args):\n",
    "    \"\"\"Process combinations for a single team in parallel\"\"\"\n",
    "    team_id, year, combo_keys, vs, ps = args\n",
    "    \n",
    "    # Ensure team_id is properly handled as string\n",
    "    team_id_str = str(team_id)\n",
    "    \n",
    "    # Load preprocessed team data\n",
    "    df, combo_to_rows = process_team_data_optimized(team_id_str, year, ps, vs)\n",
    "    \n",
    "    if df is None:\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for combo_key in combo_keys:\n",
    "        # Ensure combo_key is a string\n",
    "        combo_key_str = str(combo_key)\n",
    "        \n",
    "        if combo_key_str in combo_to_rows:\n",
    "            relevant_rows = combo_to_rows[combo_key_str]\n",
    "            df_subset = df.iloc[relevant_rows]\n",
    "            \n",
    "            result = compute_combo_stats_vectorized(df_subset, combo_key_str)\n",
    "            if result is not None:\n",
    "                # Add team metadata\n",
    "                result['team_id'] = team_id_str\n",
    "                result['year'] = year\n",
    "                result['combo_vs'] = vs\n",
    "                result['season'] = f\"{year-1}-{str(year)[-2:]}\"\n",
    "                results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_year_combinations_parallel(year, ps=False, vs=False, sizes=[2, 3, 4], max_workers=None):\n",
    "    \"\"\"Parallel processing version of get_year_combinations with separate files per size\"\"\"\n",
    "    logger.info(f\"Starting parallel combination generation for year {year}\")\n",
    "    logger.info(f\"Parameters: ps={ps}, vs={vs}, sizes={sizes}\")\n",
    "    \n",
    "    # Load index\n",
    "    index_file = 'index_master_ps.csv' if ps else 'index_master.csv'\n",
    "    index = pd.read_csv(index_file)\n",
    "    index = index.dropna(subset=['team_id'])\n",
    "    index = index[index.year == year]\n",
    "    \n",
    "    unique_teams = index['team_id'].unique()\n",
    "    logger.info(f\"Found {len(unique_teams)} unique teams for year {year}\")\n",
    "    \n",
    "    # First pass: collect all possible combinations, grouped by size\n",
    "    logger.info(\"Collecting all possible combinations...\")\n",
    "    combos_by_size = defaultdict(set)\n",
    "    \n",
    "    for team_id in tqdm(unique_teams, desc=\"Collecting combinations\"):\n",
    "        try:\n",
    "            # Ensure team_id is string\n",
    "            team_id_str = str(team_id)\n",
    "            _, combo_to_rows = process_team_data_optimized(team_id_str, year, ps, vs)\n",
    "            if combo_to_rows:\n",
    "                for combo_key in combo_to_rows.keys():\n",
    "                    combo_key_str = str(combo_key)\n",
    "                    combo_size = len(combo_key_str.split('-'))\n",
    "                    if combo_size in sizes:\n",
    "                        combos_by_size[combo_size].add(combo_key_str)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error collecting combinations for team {team_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Log combination counts by size\n",
    "    for size in sizes:\n",
    "        logger.info(f\"Found {len(combos_by_size[size])} combinations of size {size}\")\n",
    "    \n",
    "    # Process each size separately\n",
    "    results_by_size = {}\n",
    "    \n",
    "    for size in sizes:\n",
    "        if len(combos_by_size[size]) == 0:\n",
    "            logger.warning(f\"No combinations found for size {size}\")\n",
    "            results_by_size[size] = pd.DataFrame()\n",
    "            continue\n",
    "            \n",
    "        logger.info(f\"Processing size {size} combinations...\")\n",
    "        \n",
    "        # Prepare parallel processing arguments for this size\n",
    "        if max_workers is None:\n",
    "            max_workers = min(mp.cpu_count() - 1, len(unique_teams))\n",
    "        \n",
    "        size_combos = list(combos_by_size[size])\n",
    "        combo_chunks = np.array_split(size_combos, max_workers)\n",
    "        \n",
    "        args_list = []\n",
    "        for team_id in unique_teams:\n",
    "            for chunk in combo_chunks:\n",
    "                if len(chunk) > 0:\n",
    "                    # Ensure team_id is string when passing to args\n",
    "                    args_list.append((str(team_id), year, chunk.tolist(), vs, ps))\n",
    "        \n",
    "        logger.info(f\"Starting parallel processing for size {size} with {max_workers} workers\")\n",
    "        logger.info(f\"Processing {len(args_list)} work units\")\n",
    "        \n",
    "        # Process in parallel\n",
    "        size_results = []\n",
    "        \n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit all tasks\n",
    "            futures = [executor.submit(process_combinations_parallel, args) \n",
    "                      for args in args_list]\n",
    "            \n",
    "            # Collect results with progress bar\n",
    "            for future in tqdm(as_completed(futures), \n",
    "                              total=len(futures), \n",
    "                              desc=f\"Processing size {size}\"):\n",
    "                try:\n",
    "                    results = future.result()\n",
    "                    size_results.extend(results)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error in parallel processing for size {size}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        logger.info(f\"Size {size} processing complete. Generated {len(size_results)} results\")\n",
    "        \n",
    "        if size_results:\n",
    "            # Concatenate results for this size\n",
    "            size_frame = pd.concat(size_results, ignore_index=True)\n",
    "            size_frame['year'] = year\n",
    "            results_by_size[size] = size_frame\n",
    "            logger.info(f\"Size {size} dataset shape: {size_frame.shape}\")\n",
    "        else:\n",
    "            logger.warning(f\"No results generated for size {size}\")\n",
    "            results_by_size[size] = pd.DataFrame()\n",
    "    \n",
    "    return results_by_size\n",
    "\n",
    "def print_summary_stats_by_size(results_by_size, title):\n",
    "    \"\"\"Print summary statistics for results organized by size\"\"\"\n",
    "    logger.info(f\"\\n{title} Summary:\")\n",
    "    logger.info(\"-\" * 50)\n",
    "    \n",
    "    for size in sorted(results_by_size.keys()):\n",
    "        frame = results_by_size[size]\n",
    "        if not frame.empty:\n",
    "            logger.info(f\"Size {size}: {len(frame):,} combinations\")\n",
    "            logger.info(f\"  Columns: {len(frame.columns)}\")\n",
    "            logger.info(f\"  Memory usage: {frame.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB\")\n",
    "        else:\n",
    "            logger.info(f\"Size {size}: No data\")\n",
    "\n",
    "def save_results_by_size(results_by_size, year, vs=False):\n",
    "    \"\"\"Save results by size to separate CSV files\"\"\"\n",
    "    vs_suffix = \"_vs\" if vs else \"\"\n",
    "    \n",
    "    for size in sorted(results_by_size.keys()):\n",
    "        frame = results_by_size[size]\n",
    "        if not frame.empty:\n",
    "            filename = f\"{year}_combinations_size_{size}{vs_suffix}.csv\"\n",
    "            frame.to_csv(filename, index=False)\n",
    "            logger.info(f\"Saved {len(frame):,} size-{size} combinations to {filename}\")\n",
    "\n",
    "# Optimized main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up timing\n",
    "    overall_start = time.time()\n",
    "    \n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(\"STARTING OPTIMIZED BASKETBALL ANALYTICS PROCESSING\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    # Clear cache if needed (uncomment to force refresh)\n",
    "    # import shutil\n",
    "    # if os.path.exists('cache'):\n",
    "    #     shutil.rmtree('cache')\n",
    "    \n",
    "    try:\n",
    "        # Generate regular combination statistics\n",
    "        logger.info(\"Phase 1: Generating regular combination statistics...\")\n",
    "        phase1_start = time.time()\n",
    "        \n",
    "        results_combos = get_year_combinations_parallel(2025, sizes=[2, 3, 4])\n",
    "        \n",
    "        phase1_end = time.time()\n",
    "        phase1_duration = phase1_end - phase1_start\n",
    "        logger.info(f\"Phase 1 completed in {phase1_duration:.2f} seconds\")\n",
    "        \n",
    "        # Generate vs combination statistics\n",
    "        logger.info(\"Phase 2: Generating vs combination statistics...\")\n",
    "        phase2_start = time.time()\n",
    "        \n",
    "        results_combos_vs = get_year_combinations_parallel(2025, vs=True, sizes=[2, 3, 4])\n",
    "        \n",
    "        phase2_end = time.time()\n",
    "        phase2_duration = phase2_end - phase2_start\n",
    "        logger.info(f\"Phase 2 completed in {phase2_duration:.2f} seconds\")\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print_summary_stats_by_size(results_combos, \"Regular Combinations\")\n",
    "        print_summary_stats_by_size(results_combos_vs, \"VS Combinations\")\n",
    "        \n",
    "        # Save results by size\n",
    "        logger.info(\"Saving results by size...\")\n",
    "        save_start = time.time()\n",
    "        \n",
    "        save_results_by_size(results_combos, 2025, vs=False)\n",
    "        save_results_by_size(results_combos_vs, 2025, vs=True)\n",
    "        \n",
    "        save_end = time.time()\n",
    "        logger.info(f\"Results saved in {save_end - save_start:.2f} seconds\")\n",
    "        \n",
    "        # Calculate totals for final summary\n",
    "        total_regular = sum(len(frame) for frame in results_combos.values())\n",
    "        total_vs = sum(len(frame) for frame in results_combos_vs.values())\n",
    "        \n",
    "        # Final summary\n",
    "        overall_end = time.time()\n",
    "        total_duration = overall_end - overall_start\n",
    "        \n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(\"PROCESSING COMPLETE!\")\n",
    "        logger.info(f\"Total execution time: {total_duration:.2f} seconds ({total_duration/60:.2f} minutes)\")\n",
    "        logger.info(f\"Regular combinations: {total_regular:,} rows\")\n",
    "        logger.info(f\"VS combinations: {total_vs:,} rows\")\n",
    "        logger.info(f\"Total rows generated: {total_regular + total_vs:,}\")\n",
    "        \n",
    "        # Log individual file details\n",
    "        logger.info(\"\\nGenerated files:\")\n",
    "        for size in [2, 3, 4]:\n",
    "            reg_count = len(results_combos.get(size, pd.DataFrame()))\n",
    "            vs_count = len(results_combos_vs.get(size, pd.DataFrame()))\n",
    "            logger.info(f\"  Size {size}: {reg_count:,} regular, {vs_count:,} vs\")\n",
    "            if reg_count > 0:\n",
    "                logger.info(f\"    -> 2025_combinations_size_{size}.csv\")\n",
    "            if vs_count > 0:\n",
    "                logger.info(f\"    -> 2025_combinations_size_{size}_vs.csv\")\n",
    "        \n",
    "        logger.info(\"=\"*60)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal error during processing: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef86640-2de5-44ba-bdcf-cd810962212b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
