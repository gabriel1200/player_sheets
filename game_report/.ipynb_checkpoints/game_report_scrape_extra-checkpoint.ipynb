{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing date: 20241022\n",
      "Successfully pulled Overall defense data for 20241022\n",
      "Successfully pulled 3PT defense data for 20241022\n",
      "Successfully pulled 2PT defense data for 20241022\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "def format_date_to_url(date):\n",
    "    # Convert date from YYYYMMDD to datetime object\n",
    "    date_obj = datetime.strptime(str(date), '%Y%m%d')\n",
    "    \n",
    "    # Format the date as MM%2FDD%2FYYYY\n",
    "    formatted_date = date_obj.strftime('%m%%2F%d%%2F%Y')\n",
    "    \n",
    "    return formatted_date\n",
    "\n",
    "def pull_data(url):\n",
    "    headers = {\n",
    "        \"Host\": \"stats.nba.com\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"application/json, text/plain, */*\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Referer\": \"https://stats.nba.com/\",\n",
    "        \"Origin\": \"https://stats.nba.com\",\n",
    "        \"Sec-Fetch-Dest\": \"empty\",\n",
    "        \"Sec-Fetch-Mode\": \"cors\",\n",
    "        \"Sec-Fetch-Site\": \"same-origin\",\n",
    "    }\n",
    "\n",
    "    json = requests.get(url, headers=headers).json()\n",
    "\n",
    "    if len(json[\"resultSets\"]) == 1:\n",
    "        data = json[\"resultSets\"][0][\"rowSet\"]\n",
    "        columns = json[\"resultSets\"][0][\"headers\"]\n",
    "        df = pd.DataFrame.from_records(data, columns=columns)\n",
    "    else:\n",
    "        data = json[\"resultSets\"][\"rowSet\"]\n",
    "        columns = json[\"resultSets\"][\"headers\"][1]['columnNames']\n",
    "        df = pd.DataFrame.from_records(data, columns=columns)\n",
    "\n",
    "    time.sleep(.1)  # Respect rate limits\n",
    "    return df\n",
    "\n",
    "def pull_additional_data(date_num, season, stype='Regular%20Season'):\n",
    "    \"\"\"\n",
    "    Pull additional data from new endpoints for a specific date\n",
    "    \"\"\"\n",
    "    date = format_date_to_url(date_num)\n",
    "    \n",
    "    additional_data_frames = []\n",
    "    year=int(season.split('-')[0])+1\n",
    "    \n",
    "    # Get data from your additional links\n",
    "    # Link 1: Overall defense stats\n",
    "    ps=False\n",
    "    if stype=='Playoffs':\n",
    "        ps=True\n",
    "    passed=True\n",
    "    url1 = f'https://stats.nba.com/stats/leaguedashptdefend?College=&Conference=&Country=&DateFrom={date}&DateTo={date}&DefenseCategory=Overall&Division=&DraftPick=&DraftYear=&GameSegment=&Height=&ISTRound=&LastNGames=0&LeagueID=00&Location=&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PerMode=PerGame&Period=0&PlayerExperience=&PlayerPosition=&Season={season}&SeasonSegment=&SeasonType={stype}&StarterBench=&TeamID=0&VsConference=&VsDivision=&Weight='\n",
    "    try:\n",
    "        df1 = pull_data(url1)\n",
    "        df1.rename(columns={'CLOSE_DEF_PERSON_ID': 'PLAYER_ID'}, inplace=True)\n",
    "        df1.rename(columns={col: f'overall_def_{col}' for col in df1.columns if col != 'PLAYER_ID'}, inplace=True)\n",
    "        additional_data_frames.append(df1)\n",
    "        print(f\"Successfully pulled Overall defense data for {date_num}\")\n",
    "    except Exception as e:\n",
    "        passed=False\n",
    "        print(f\"Error pulling Overall defense data for {date_num}: {str(e)}\")\n",
    "    \n",
    "    # Link 2: 3-pointers defense stats\n",
    "    url2 = f'https://stats.nba.com/stats/leaguedashptdefend?College=&Conference=&Country=&DateFrom={date}&DateTo={date}&DefenseCategory=3%20Pointers&Division=&DraftPick=&DraftYear=&GameSegment=&Height=&ISTRound=&LastNGames=0&LeagueID=00&Location=&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PerMode=PerGame&Period=0&PlayerExperience=&PlayerPosition=&Season={season}&SeasonSegment=&SeasonType={stype}&StarterBench=&TeamID=0&VsConference=&VsDivision=&Weight='\n",
    "    try:\n",
    "        df2 = pull_data(url2)\n",
    "        df2.rename(columns={'CLOSE_DEF_PERSON_ID': 'PLAYER_ID'}, inplace=True)\n",
    "        df2.rename(columns={col: f'three_pt_def_{col}' for col in df2.columns if col != 'PLAYER_ID'}, inplace=True)\n",
    "        additional_data_frames.append(df2)\n",
    "        print(f\"Successfully pulled 3PT defense data for {date_num}\")\n",
    "    except Exception as e:\n",
    "        passed=False\n",
    "        print(f\"Error pulling 3PT defense data for {date_num}: {str(e)}\")\n",
    "    \n",
    "    # Link 3: 2-pointers defense stats\n",
    "    url3 = f'https://stats.nba.com/stats/leaguedashptdefend?College=&Conference=&Country=&DateFrom={date}&DateTo={date}&DefenseCategory=2%20Pointers&Division=&DraftPick=&DraftYear=&GameSegment=&Height=&ISTRound=&LastNGames=0&LeagueID=00&Location=&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PerMode=PerGame&Period=0&PlayerExperience=&PlayerPosition=&Season={season}&SeasonSegment=&SeasonType={stype}&StarterBench=&TeamID=0&VsConference=&VsDivision=&Weight='\n",
    "    try:\n",
    "        df3 = pull_data(url3)\n",
    "        df3.rename(columns={'CLOSE_DEF_PERSON_ID': 'PLAYER_ID'}, inplace=True)\n",
    "        df3.rename(columns={col: f'two_pt_def_{col}' for col in df3.columns if col != 'PLAYER_ID'}, inplace=True)\n",
    "        additional_data_frames.append(df3)\n",
    "        print(f\"Successfully pulled 2PT defense data for {date_num}\")\n",
    "    except Exception as e:\n",
    "        passed=False\n",
    "        print(f\"Error pulling 2PT defense data for {date_num}: {str(e)}\")\n",
    "    \n",
    "    # Link 4: Less than 6ft defense stats\n",
    "    url4 = f'https://stats.nba.com/stats/leaguedashptdefend?College=&Conference=&Country=&DateFrom={date}&DateTo={date}&DefenseCategory=Less%20Than%206Ft&Division=&DraftPick=&DraftYear=&GameSegment=&Height=&ISTRound=&LastNGames=0&LeagueID=00&Location=&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PerMode=PerGame&Period=0&PlayerExperience=&PlayerPosition=&Season={season}&SeasonSegment=&SeasonType={stype}&StarterBench=&TeamID=0&VsConference=&VsDivision=&Weight='\n",
    "    try:\n",
    "        df4 = pull_data(url4)\n",
    "        df4.rename(columns={'CLOSE_DEF_PERSON_ID': 'PLAYER_ID'}, inplace=True)\n",
    "        df4.rename(columns={col: f'less_6ft_def_{col}' for col in df4.columns if col != 'PLAYER_ID'}, inplace=True)\n",
    "        additional_data_frames.append(df4)\n",
    "        print(f\"Successfully pulled <6ft defense data for {date_num}\")\n",
    "    except Exception as e:\n",
    "        passed=False\n",
    "        print(f\"Error pulling <6ft defense data for {date_num}: {str(e)}\")\n",
    "    \n",
    "    # Link 5: Less than 10ft defense stats\n",
    "    url5 = f'https://stats.nba.com/stats/leaguedashptdefend?College=&Conference=&Country=&DateFrom={date}&DateTo={date}&DefenseCategory=Less%20Than%2010Ft&Division=&DraftPick=&DraftYear=&GameSegment=&Height=&ISTRound=&LastNGames=0&LeagueID=00&Location=&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PerMode=PerGame&Period=0&PlayerExperience=&PlayerPosition=&Season={season}&SeasonSegment=&SeasonType={stype}&StarterBench=&TeamID=0&VsConference=&VsDivision=&Weight='\n",
    "    try:\n",
    "        df5 = pull_data(url5)\n",
    "        df5.rename(columns={'CLOSE_DEF_PERSON_ID': 'PLAYER_ID'}, inplace=True)\n",
    "        df5.rename(columns={col: f'less_10ft_def_{col}' for col in df5.columns if col != 'PLAYER_ID'}, inplace=True)\n",
    "        additional_data_frames.append(df5)\n",
    "        print(f\"Successfully pulled <10ft defense data for {date_num}\")\n",
    "    except Exception as e:\n",
    "        passed=False\n",
    "        print(f\"Error pulling <10ft defense data for {date_num}: {str(e)}\")\n",
    "    \n",
    "    # Link 6: Less than 15ft defense stats\n",
    "    url6 = f'https://stats.nba.com/stats/leaguedashptdefend?College=&Conference=&Country=&DateFrom={date}&DateTo={date}&DefenseCategory=Greater%20Than%2015Ft&Division=&DraftPick=&DraftYear=&GameSegment=&Height=&ISTRound=&LastNGames=0&LeagueID=00&Location=&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PerMode=PerGame&Period=0&PlayerExperience=&PlayerPosition=&Season={season}&SeasonSegment=&SeasonType={stype}&StarterBench=&TeamID=0&VsConference=&VsDivision=&Weight='\n",
    "    try:\n",
    "        df6 = pull_data(url6)\n",
    "        df6.rename(columns={'CLOSE_DEF_PERSON_ID': 'PLAYER_ID'}, inplace=True)\n",
    "        df6.rename(columns={col: f'more_15ft_def_{col}' for col in df6.columns if col != 'PLAYER_ID'}, inplace=True)\n",
    "        additional_data_frames.append(df6)\n",
    "        print(f\"Successfully pulled >15ft defense data for {date_num}\")\n",
    "    except Exception as e:\n",
    "        passed=False\n",
    "        print(f\"Error pulling <15ft defense data for {date_num}: {str(e)}\")\n",
    "    \n",
    "    # Link 7: Hustle stats\n",
    "    if year>=2016:\n",
    "        url7 = f'https://stats.nba.com/stats/leaguehustlestatsplayer?College=&Conference=&Country=&DateFrom={date}&DateTo={date}&Division=&DraftPick=&DraftYear=&GameScope=&Height=&ISTRound=&LastNGames=0&LeagueID=00&Location=&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PaceAdjust=N&PerMode=PerGame&PlayerExperience=&PlayerPosition=&PlusMinus=N&Rank=N&Season={season}&SeasonSegment=&SeasonType={stype}&TeamID=0&VsConference=&VsDivision=&Weight='\n",
    "        try:\n",
    "            df7 = pull_data(url7)\n",
    "            df7.rename(columns={col: f'hustle_{col}' for col in df7.columns if col != 'PLAYER_ID'}, inplace=True)\n",
    "            additional_data_frames.append(df7)\n",
    "            print(f\"Successfully pulled hustle stats for {date_num}\")\n",
    "        except Exception as e:\n",
    "            passed=False\n",
    "            print(f\"Error pulling hustle stats for {date_num}: {str(e)}\")\n",
    "    \n",
    "    # Link 8: Post touch stats\n",
    "    url8 = f'https://stats.nba.com/stats/leaguedashptstats?College=&Conference=&Country=&DateFrom={date}&DateTo={date}&Division=&DraftPick=&DraftYear=&GameScope=&Height=&ISTRound=&LastNGames=0&LeagueID=00&Location=&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PerMode=PerGame&PlayerExperience=&PlayerOrTeam=Player&PlayerPosition=&PtMeasureType=PostTouch&Season={season}&SeasonSegment=&SeasonType={stype}&StarterBench=&TeamID=0&VsConference=&VsDivision=&Weight='\n",
    "    try:\n",
    "        df8 = pull_data(url8)\n",
    "        df8.rename(columns={col: f'post_touch_{col}' for col in df8.columns if col != 'PLAYER_ID'}, inplace=True)\n",
    "        additional_data_frames.append(df8)\n",
    "        print(f\"Successfully pulled post touch stats for {date_num}\")\n",
    "    except Exception as e:\n",
    "        passed=False\n",
    "        print(f\"Error pulling post touch stats for {date_num}: {str(e)}\")\n",
    "    \n",
    "    # Link 9: Speed distance stats\n",
    "    url9 = f'https://stats.nba.com/stats/leaguedashptstats?College=&Conference=&Country=&DateFrom={date}&DateTo={date}&Division=&DraftPick=&DraftYear=&GameScope=&Height=&ISTRound=&LastNGames=0&LeagueID=00&Location=&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PerMode=PerGame&PlayerExperience=&PlayerOrTeam=Player&PlayerPosition=&PtMeasureType=SpeedDistance&Season={season}&SeasonSegment=&SeasonType={stype}&StarterBench=&TeamID=0&VsConference=&VsDivision=&Weight='\n",
    "    try:\n",
    "        df9 = pull_data(url9)\n",
    "        df9.rename(columns={col: f'speed_distance_{col}' for col in df9.columns if col != 'PLAYER_ID'}, inplace=True)\n",
    "        additional_data_frames.append(df9)\n",
    "        print(f\"Successfully pulled speed distance stats for {date_num}\")\n",
    "    except Exception as e:\n",
    "        passed=False\n",
    "        print(f\"Error pulling speed distance stats for {date_num}: {str(e)}\")\n",
    "    \n",
    "    # Merge all the additional data frames on PLAYER_ID\n",
    "    result_df = additional_data_frames[0]\n",
    "    for df in additional_data_frames[1:]:\n",
    "        result_df = result_df.merge(df, on='PLAYER_ID', how='outer')\n",
    "    \n",
    "    # Add date and season information\n",
    "    result_df['date'] = date_num\n",
    "    result_df['season'] = season.replace('-', '_')\n",
    "    \n",
    "    return result_df,passed\n",
    "\n",
    "def update_game_files(start_year, end_year, stype='Regular%20Season'):\n",
    "    \"\"\"\n",
    "    Update existing game files with additional data for a range of seasons\n",
    "    \"\"\"\n",
    "    # Get all dates for which we have existing data\n",
    "    for year in range(start_year, end_year):\n",
    "        season = f\"{year-1}-{str(year)[-2:]}\"\n",
    "        \n",
    "        # Create a directory for additional data if it doesn't exist\n",
    "        additional_data_dir = f'additional_data/{year}'\n",
    "        if not os.path.exists(additional_data_dir):\n",
    "            os.makedirs(additional_data_dir, exist_ok=True)\n",
    "        \n",
    "        # Get list of game files we already have\n",
    "        game_files = glob.glob(f'{year}/*.csv')\n",
    "        \n",
    "        # Extract unique dates from existing files\n",
    "        existing_dates = set()\n",
    "        date_to_games = {}\n",
    "        \n",
    "        # Read the date information from existing files\n",
    "        for game_file in game_files:\n",
    "            try:\n",
    "                game_id = os.path.basename(game_file).replace('.csv', '')\n",
    "                game_df = pd.read_csv(game_file)\n",
    "                \n",
    "                if 'date' in game_df.columns:\n",
    "                    date = game_df['date'].iloc[0]\n",
    "                    existing_dates.add(date)\n",
    "                    \n",
    "                    if date not in date_to_games:\n",
    "                        date_to_games[date] = []\n",
    "                    date_to_games[date].append(game_id)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {game_file}: {str(e)}\")\n",
    "        \n",
    "        # Process each date to get additional data\n",
    "        date_dict={\n",
    "                2014: 20140419,\n",
    "                2015: 20150418,\n",
    "                2016: 20160416,\n",
    "                2017: 20170415,\n",
    "                2018: 20180414,\n",
    "                2019: 20190413,\n",
    "                2020: 20200817,  # Delayed due to COVID-19\n",
    "                2021: 20210522,\n",
    "                2022: 20220416,\n",
    "                2023: 20230415,\n",
    "                2024: 20240420,\n",
    "                2025: 20250420\n",
    "            }\n",
    "        year=int(season.split('-')[0])\n",
    "        year+=1\n",
    "        cutoff= date_dict[year]\n",
    "  \n",
    "        for date in sorted(existing_dates):\n",
    "            additional_data_file = f'{additional_data_dir}/{date}_additional.csv'\n",
    "            \n",
    "            # Skip if we already have the additional data file\n",
    "            if os.path.exists(additional_data_file):\n",
    "                print(f\"Additional data file already exists for date {date}. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"Processing date: {date}\")\n",
    "            \n",
    "            try:\n",
    "                # Pull additional data\n",
    "                if date>=cutoff:\n",
    "                    season_type='Playoffs'\n",
    "                else:\n",
    "                    season_type='Regular%20Season'\n",
    "                additional_df,passed = pull_additional_data(date, season, season_type)\n",
    "                \n",
    "                # Save the additional data for this date\n",
    "                if passed:\n",
    "                    additional_df.to_csv(additional_data_file, index=False)\n",
    "                    print(f\"Saved additional data for date {date}\")\n",
    "                    \n",
    "                    # Now update each game file for this date with the additional data\n",
    "                    game_ids = date_to_games.get(date, [])\n",
    "                    \n",
    "                    for game_id in game_ids:\n",
    "                        game_file = f'{year}/{game_id}.csv'\n",
    "                        \n",
    "                        try:\n",
    "                            # FIX: Actually load the game file data here!\n",
    "                            game_df = pd.read_csv(game_file)\n",
    "                            \n",
    "                            # Identify columns to keep (merge keys) and remove overlapping columns\n",
    "                            cols_to_keep = ['PLAYER_ID', 'date']\n",
    "                            cols_to_remove = [col for col in game_df.columns if col in additional_df.columns and col not in cols_to_keep]\n",
    "\n",
    "                            # Drop the overlapping columns\n",
    "                            game_df = game_df.drop(columns=cols_to_remove)\n",
    "\n",
    "                            # Merge with additional data\n",
    "                            updated_df = game_df.merge(additional_df, on=cols_to_keep, how='left')\n",
    "\n",
    "                            # Save the updated game file\n",
    "                            updated_df.to_csv(f'{year}/{game_id}_updated.csv', index=False)\n",
    "                            print(f\"Updated game file: {game_id}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error updating game file {game_id}: {str(e)}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing additional data for date {date}: {str(e)}\")\n",
    "\n",
    "def update_all_games_file(year):\n",
    "    \"\"\"\n",
    "    Update the consolidated 'all_games' file with additional data\n",
    "    \"\"\"\n",
    "    # Get all updated game files\n",
    "    updated_game_files = glob.glob(f'{year}/*_updated.csv')\n",
    "    \n",
    "    if not updated_game_files:\n",
    "        print(f\"No updated game files found for year {year}\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    # Combine all updated game files\n",
    "    all_updated_games = []\n",
    "    \n",
    "    for game_file in updated_game_files:\n",
    "        try:\n",
    "            game_df = pd.read_csv(game_file)\n",
    "            all_updated_games.append(game_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading updated game file {game_file}: {str(e)}\")\n",
    "    \n",
    "    if all_updated_games:\n",
    "        # Concatenate all game data\n",
    "        all_games_df = pd.concat(all_updated_games, ignore_index=True)\n",
    "        \n",
    "        # Save as CSV and parquet\n",
    "        all_games_df.to_csv(f'all_games/all_{year}_updated.csv', index=False)\n",
    "        all_games_df.to_parquet(f'all_games/all_{year}_updated.parquet', index=False)\n",
    "        print(f\"Updated consolidated files for year {year}\")\n",
    "    else:\n",
    "        print(f\"No valid updated game files to concatenate for year {year}\")\n",
    "\n",
    "def cleanup_and_rename(year):\n",
    "    \"\"\"\n",
    "    After successful update, rename updated files to replace originals\n",
    "    \"\"\"\n",
    "    # Rename individual game files\n",
    "    updated_game_files = glob.glob(f'{year}/*_updated.csv')\n",
    "    \n",
    "    for game_file in updated_game_files:\n",
    "        original_file = game_file.replace('_updated.csv', '.csv')\n",
    "        os.rename(game_file, original_file)\n",
    "        print(f\"Replaced {original_file} with updated version\")\n",
    "    \n",
    "    # Rename consolidated files\n",
    "    if os.path.exists(f'all_games/all_{year}_updated.csv'):\n",
    "        os.rename(f'all_games/all_{year}_updated.csv', f'all_games/all_{year}.csv')\n",
    "        print(f\"Replaced all_games/all_{year}.csv with updated version\")\n",
    "    \n",
    "    if os.path.exists(f'all_games/all_{year}_updated.parquet'):\n",
    "        os.rename(f'all_games/all_{year}_updated.parquet', f'all_games/all_{year}.parquet')\n",
    "        print(f\"Replaced all_games/all_{year}.parquet with updated version\")\n",
    "\n",
    "def main():\n",
    "    # Define year range\n",
    "    start_year = 2025\n",
    "    end_year = 2026\n",
    "    \n",
    "    # Update game files with additional data\n",
    "    update_game_files(start_year, end_year)\n",
    "    \n",
    "    # Update consolidated files\n",
    "    for year in range(start_year, end_year):\n",
    "        update_all_games_file(year)\n",
    "    \n",
    "    # Clean up and rename files\n",
    "    for year in range(start_year, end_year):\n",
    "        cleanup_and_rename(year)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
